{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# ==== Mandatory Code (DO NOT CHANGE) ====\n",
    "def set_seed(seed=13):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(13)\n",
    "\n",
    "def count_cpgs(seq: str) -> int:\n",
    "    cgs = 0\n",
    "    for i in range(0, len(seq) - 1):\n",
    "        dimer = seq[i:i+2]\n",
    "        if dimer == \"CG\":\n",
    "            cgs += 1\n",
    "    return cgs\n",
    "\n",
    "alphabet = 'NACGT'\n",
    "dna2int = {a: i for a, i in zip(alphabet, range(5))}\n",
    "int2dna = {i: a for a, i in zip(alphabet, range(5))}\n",
    "\n",
    "intseq_to_dnaseq = partial(map, int2dna.get)\n",
    "dnaseq_to_intseq = partial(map, dna2int.get)\n",
    "\n",
    "# ==== Device Setup ====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==== Attention Class ====\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2 + hidden_dim * 2, hidden_dim)  # [h;e] -> hidden_dim\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = encoder_outputs.size(0)\n",
    "        batch_size = encoder_outputs.size(1)\n",
    "        hidden = hidden.unsqueeze(0).repeat(seq_len, 1, 1)  # [seq_len, batch_size, hidden_dim * 2]\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [seq_len, batch_size, hidden_dim]\n",
    "        attention = self.v(energy).squeeze(2)  # [seq_len, batch_size]\n",
    "        return F.softmax(attention, dim=0)  # [seq_len, batch_size]\n",
    "\n",
    "# ==== CpGPredictor Class ====\n",
    "class CpGPredictor(nn.Module):\n",
    "    def __init__(self, input_dim=5, embedding_dim=64, hidden_dim=256, num_layers=2, dropout=0.33262129231366233):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=0)\n",
    "        self.layernorm = nn.LayerNorm(embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True,\n",
    "                            dropout=dropout if num_layers > 1 else 0, bidirectional=True)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.layernorm(embedded)\n",
    "        packed_embedded = torch.nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hidden, _) = self.lstm(packed_embedded)\n",
    "        lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        hidden = torch.cat((hidden[-2], hidden[-1]), dim=-1)  # Concatenate forward and backward hidden states\n",
    "        attn_weights = self.attention(hidden, lstm_out.transpose(0, 1))\n",
    "        attn_weights = attn_weights.transpose(0, 1).unsqueeze(1)\n",
    "        context = torch.bmm(attn_weights, lstm_out)\n",
    "        \n",
    "        out = self.dropout(context.squeeze(1))\n",
    "        prediction = self.classifier(out).squeeze()\n",
    "        return context.squeeze(1), prediction, attn_weights.squeeze(1)\n",
    "\n",
    "# ==== Prediction Function ====\n",
    "def predict_cpg(model, sequence, return_attention=False):\n",
    "    \"\"\"\n",
    "    Predict CG count for a given DNA sequence and optionally return attention weights.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Trained CpGPredictor model.\n",
    "        sequence (str): DNA sequence as a string (e.g., \"ATGCGCGTANCGCGAT\").\n",
    "        return_attention (bool): Whether to return attention weights.\n",
    "    \n",
    "    Returns:\n",
    "        float or tuple: Predicted CG count, and optionally attention weights.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    int_seq = list(dnaseq_to_intseq(sequence.upper()))\n",
    "    x = torch.tensor([int_seq], dtype=torch.long).to(device)\n",
    "    lengths = torch.tensor([len(int_seq)], dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, pred, attn_weights = model(x, lengths)\n",
    "    pred = pred.item()\n",
    "    if return_attention:\n",
    "        return pred, attn_weights.squeeze().cpu().numpy()\n",
    "    return pred\n",
    "\n",
    "# ==== Testing Function ====\n",
    "def test_model(model_path, test_sequences):\n",
    "    \"\"\"\n",
    "    Test the trained model on new unseen sequences.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model weights (e.g., \"bilstm_bahdanau_final.pt\").\n",
    "        test_sequences (list): List of DNA sequences as strings to test.\n",
    "    \"\"\"\n",
    "    # Initialize model with the same architecture used during training\n",
    "    model = CpGPredictor(\n",
    "        input_dim=5, \n",
    "        embedding_dim=64, \n",
    "        hidden_dim=256, \n",
    "        num_layers=2, \n",
    "        dropout=0.3).to(device)\n",
    "    \n",
    "    '''\n",
    "    Best_hyperparameters: {\n",
    "    'embedding_dim': 64, \n",
    "    'hidden_dim': 256, \n",
    "    'num_layers': 2, \n",
    "    'dropout': 0.33262129231366233, \n",
    "    'lr': 0.0008572034020671933, \n",
    "    'weight_decay': 3.4321573869941195e-06, \n",
    "    'batch_size': 32}\n",
    "    '''\n",
    "    \n",
    "    # Load the trained weights\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file {model_path} not found. Please train the model first.\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    print(f\"Loaded model from {model_path}\")\n",
    "\n",
    "    # Test each sequence\n",
    "    for seq in test_sequences:\n",
    "        actual_cg = count_cpgs(seq)\n",
    "        pred_cg, attn_weights = predict_cpg(model, seq, return_attention=True)\n",
    "        \n",
    "        print(f\"\\nSequence: {seq}\")\n",
    "        print(f\"Actual CG Count: {actual_cg}\")\n",
    "        print(f\"Predicted CG Count: {pred_cg:.2f}\")\n",
    "        print(f\"Absolute Error: {abs(actual_cg - pred_cg):.2f}\")\n",
    "\n",
    "        # Visualize attention weights\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.bar(range(len(attn_weights)), attn_weights)\n",
    "        plt.title(f\"Attention Weights for Sequence: {seq[:20]}... (Actual: {actual_cg}, Predicted: {pred_cg:.2f})\")\n",
    "        plt.xlabel(\"Position\")\n",
    "        plt.ylabel(\"Attention Weight\")\n",
    "        plt.savefig(f\"attention_{seq[:10]}.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Prediction for ATGCGCGTANCGCCGNCCGGCGCGTANCTACGGCGCGTANCCGCGTANCGCCGNCCGGCGCGTANCTANCGCGGCGCGTAGCGTANCCGCGTANNCCGCGTANCAT: 0.01, Actual: 26\n"
     ]
    }
   ],
   "source": [
    "  # Example prediction and attention visualization\n",
    "test_seq = \"ATGCGCGTANCGCCGNCCGGCGCGTANCTACGGCGCGTANCCGCGTANCGCCGNCCGGCGCGTANCTANCGCGGCGCGTAGCGTANCCGCGTANNCCGCGTANCAT\"\n",
    "pred, attn = predict_cpg(model, test_seq, return_attention=True)\n",
    "actual = count_cpgs(test_seq)\n",
    "logging.info(f\"Prediction for {test_seq}: {pred:.2f}, Actual: {actual}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\\WORKING STREMLIT PROJECTS\\CpG_Predictor_App\\test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anilj.ANIL_JOSEPH\\OneDrive\\Desktop\\final_dna_testing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_dir = r'c:\\Users\\anilj.ANIL_JOSEPH\\OneDrive\\Desktop\\final_dna_testing'\n",
    "os.chdir(root_dir)\n",
    "\n",
    "print(os.getcwd())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from ./models/bilstm_bahdanau_final.pt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'function' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_sequences\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 144\u001b[0m, in \u001b[0;36mtest_model\u001b[1;34m(model_path, test_sequences)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Test each sequence\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m test_sequences:\n\u001b[0;32m    145\u001b[0m     actual_cg \u001b[38;5;241m=\u001b[39m count_cpgs(seq)\n\u001b[0;32m    146\u001b[0m     pred_cg, attn_weights \u001b[38;5;241m=\u001b[39m predict_cpg(model, seq, return_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'function' object is not iterable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_path = './models/bilstm_bahdanau_final.pt'\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Model file {model_path} not found.\")\n",
    "else:\n",
    "    test_model(model_path, test_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from ./models/bilstm_bahdanau_final.pt\n",
      "\n",
      "Sequence: ATGCGCGTANCGCGAT\n",
      "Actual CG Count: 4\n",
      "Predicted CG Count: 23.74\n",
      "Absolute Error: 19.74\n",
      "\n",
      "Sequence: CGCGATCGCGATCGCGATCGCGAT\n",
      "Actual CG Count: 8\n",
      "Predicted CG Count: 24.74\n",
      "Absolute Error: 16.74\n",
      "\n",
      "Sequence: ATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGC\n",
      "Actual CG Count: 0\n",
      "Predicted CG Count: 0.07\n",
      "Absolute Error: 0.07\n",
      "\n",
      "Sequence: NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATAT\n",
      "Actual CG Count: 50\n",
      "Predicted CG Count: 14.39\n",
      "Absolute Error: 35.61\n"
     ]
    }
   ],
   "source": [
    "# ==== Main Execution ====\n",
    "if __name__ == \"__main__\":\n",
    "    # Example unseen test sequences (you can modify these)\n",
    "    test_sequences = [\n",
    "        \"ATGCGCGTANCGCGAT\",           # Short sequence\n",
    "        \"CGCGATCGCGATCGCGATCGCGAT\",   # High CG density\n",
    "        \"ATGC\" * 50,                    # Repeated pattern, length 200\n",
    "        \"N\" * 100 + \"CG\" * 50 + \"AT\" * 50,  # Mixed pattern, length 200\n",
    "    ]\n",
    "\n",
    "    # Path to the trained model\n",
    "    model_path = r\"./models/bilstm_bahdanau_final.pt\"\n",
    "\n",
    "    # Run the test\n",
    "    try:\n",
    "        test_model(model_path, test_sequences)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during testing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Model file ./models/bilstm_bahdanau_final.pt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Load the trained model weights\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model_path, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[0;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Model file ./models/bilstm_bahdanau_final.pt not found."
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# ==== Load Trained Model ====\n",
    "model = CpGPredictor(\n",
    "    input_dim=5, \n",
    "    embedding_dim=64, \n",
    "    hidden_dim=256, \n",
    "    num_layers=2, \n",
    "    dropout=0.33262129231366233).to(device)\n",
    "\n",
    "model_path = r\"./models/bilstm_bahdanau_final.pt\"  # Update this if your model filename is different\n",
    "\n",
    "# Load the trained model weights\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Model file {model_path} not found.\")\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "logging.info(\"Model loaded successfully.\")\n",
    "\n",
    "# ==== Example Sequence ====\n",
    "test_seq = \"ATGCGCGTANCGGGCGCCCGCGTANCATCCGCGTANCGCCGNCCGGCGCGTANCTANCGCGGCGCGTANCCGCGTANCGCCGNCCGGCGCGTANCTANCGCGGCGCGTAGCGTANCCGCGTANNCCGCGTANCATCCGCGTANCGCCGNCCGGCGCGTANCTANCGCGGCGCGTAGCGTANCCGCCCGNCCGGCGCGTANCTACGGCGCCCGCGTANCATCCGCGTANCGCCGNCCGGCGCGTANCTANCGCGGCGCGTANCCGCGTANCGCCGNCCGGCGCGTANCTANCGCGGCGCGTAGCGTANCCGCGTANNCCGCGTANCATCCGCGTANCGCCGNCCGGCGCGTANCTANCGCGGCGCGTAGCGTANCCGC\"\n",
    "\n",
    "# ==== Prediction ====\n",
    "pred, attn = predict_cpg(model, test_seq, return_attention=True)\n",
    "actual = count_cpgs(test_seq)\n",
    "\n",
    "# ==== Logging and Visualization ====\n",
    "logging.info(f\"Sequence: {test_seq}\")\n",
    "logging.info(f\"Actual CpG count: {actual}\")\n",
    "logging.info(f\"Predicted CpG count: {pred:.2f}\")\n",
    "logging.info(f\"Absolute Error: {abs(pred - actual):.2f}\")\n",
    "\n",
    "# ==== Attention Visualization ====\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.bar(range(len(attn)), attn)\n",
    "plt.title(f\"Attention Weights for Input Sequence\\nActual: {actual}, Predicted: {pred:.2f}\")\n",
    "plt.xlabel(\"Nucleotide Position\")\n",
    "plt.ylabel(\"Attention Weight\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"attention_sample.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_sample_sequence(\n",
    "    sequence: str,\n",
    "    model_path: str = r\"./models/bilstm_bahdanau_final.pt\" ,\n",
    "    visualize_attention: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads the trained model, predicts CpG count for a given sequence, and optionally visualizes attention weights.\n",
    "\n",
    "    Args:\n",
    "        sequence (str): DNA sequence to test.\n",
    "        model_path (str): Path to the saved model weights.\n",
    "        visualize_attention (bool): Whether to display attention plot.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    # Model hyperparameters - must match training time\n",
    "    model = CpGPredictor(\n",
    "        input_dim=5,\n",
    "        embedding_dim=64,\n",
    "        hidden_dim=256,\n",
    "        num_layers=2,\n",
    "        dropout=0.33262129231366233\n",
    "    ).to(device)\n",
    "\n",
    "    # Load model weights\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file '{model_path}' not found.\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    logging.info(\"✅ Model loaded successfully.\")\n",
    "\n",
    "    # Predict\n",
    "    pred, attn = predict_cpg(model, sequence, return_attention=True)\n",
    "    actual = count_cpgs(sequence)\n",
    "\n",
    "    # Logging\n",
    "    logging.info(f\"📌 Sequence: {sequence}\")\n",
    "    logging.info(f\"✅ Actual CpG count: {actual}\")\n",
    "    logging.info(f\"🧠 Predicted CpG count: {pred:.2f}\")\n",
    "    logging.info(f\"❗ Absolute Error: {abs(pred - actual):.2f}\")\n",
    "\n",
    "    # Visualization\n",
    "    if visualize_attention:\n",
    "        plt.figure(figsize=(14, 4))\n",
    "        plt.bar(range(len(attn)), attn)\n",
    "        plt.title(f\"Attention Weights for Input Sequence\\nActual: {actual}, Predicted: {pred:.2f}\")\n",
    "        plt.xlabel(\"Nucleotide Position\")\n",
    "        plt.ylabel(\"Attention Weight\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"attention_sample.png\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect idea! Let’s extend the function so it can:\n",
    "\n",
    "✅ Handle multiple sequences\n",
    "\n",
    "✅ Save predictions and attention plot to files\n",
    "\n",
    "✅ Write a dedicated test log to ./logs/testing/ directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def test_sequences(\n",
    "    sequences,\n",
    "    model_path=r\"./models/bilstm_bahdanau_final.pt\" ,\n",
    "    output_dir=\"./logs/testing/testing_image\",\n",
    "    log_dir=\"./logs/testing/testing_logs\",\n",
    "    save_attention=True,\n",
    "    save_csv=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Predicts CpG counts for a list of sequences using a pre-trained model and logs results.\n",
    "\n",
    "    Args:\n",
    "        sequences (list[str]): List of DNA sequences.\n",
    "        model_path (str): Path to the saved model.\n",
    "        output_dir (str): Directory to save attention plots.\n",
    "        log_dir (str): Directory to save log file.\n",
    "        save_attention (bool): Whether to save attention bar plots.\n",
    "        save_csv (bool): Whether to save results in CSV format.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    # Setup timestamped logging\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_file = os.path.join(log_dir, f\"test_log_{timestamp}.log\")\n",
    "    logging.basicConfig(\n",
    "        filename=log_file,\n",
    "        filemode=\"w\",\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "\n",
    "    # Load the model\n",
    "    model = CpGPredictor(\n",
    "        input_dim=5,\n",
    "        embedding_dim=64,\n",
    "        hidden_dim=256,\n",
    "        num_layers=2,\n",
    "        dropout=0.33262129231366233\n",
    "    ).to(device)\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file '{model_path}' not found.\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    logging.info(\"✅ Model loaded successfully.\")\n",
    "\n",
    "    # For CSV logging\n",
    "    results = []\n",
    "\n",
    "    for idx, seq in enumerate(sequences, 1):\n",
    "        pred, attn = predict_cpg(model, seq, return_attention=True)\n",
    "        actual = count_cpgs(seq)\n",
    "        error = abs(pred - actual)\n",
    "\n",
    "        logging.info(f\"\\nSequence {idx}:\")\n",
    "        logging.info(f\"📌 {seq}\")\n",
    "        logging.info(f\"✅ Actual CpG: {actual}, 🧠 Predicted CpG: {pred:.2f}, ❗ Error: {error:.2f}\")\n",
    "\n",
    "        results.append({\n",
    "            \"Sequence_ID\": f\"Seq_{idx}\",\n",
    "            \"Sequence\": seq,\n",
    "            \"Actual_CpG\": actual,\n",
    "            \"Predicted_CpG\": round(pred, 2),\n",
    "            \"Error\": round(error, 2)\n",
    "        })\n",
    "\n",
    "        if save_attention:\n",
    "            plt.figure(figsize=(14, 4))\n",
    "            plt.bar(range(len(attn)), attn)\n",
    "            plt.title(f\"Attention - Seq {idx}\\nActual: {actual}, Predicted: {pred:.2f}\")\n",
    "            plt.xlabel(\"Nucleotide Position\")\n",
    "            plt.ylabel(\"Attention Weight\")\n",
    "            plt.tight_layout()\n",
    "            plot_path = os.path.join(output_dir, f\"attention_seq_{idx}.png\")\n",
    "            plt.savefig(plot_path)\n",
    "            plt.close()\n",
    "            logging.info(f\"🖼️ Attention plot saved to: {plot_path}\")\n",
    "\n",
    "    # Save results to CSV\n",
    "    if save_csv:\n",
    "        df = pd.DataFrame(results)\n",
    "        csv_path = os.path.join(output_dir, f\"prediction_results_{timestamp}.csv\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        logging.info(f\"📝 Predictions saved to: {csv_path}\")\n",
    "\n",
    "    print(f\"✅ Testing complete. Logs: {log_file}, Results: {csv_path if save_csv else 'Not saved'}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Model file './models/bilstm_bahdanau_final.pt' not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m test_seqs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mATGCGCGTANCGCCGNCCGGCGCGTANCTACGGCGCGTANCCGCGTANCGCCGNCCGGCGCGTANCTANCGCGGCGCGTAGCGTANCCGCGTANNCCGCGTANCAT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCGTANCGCGCGTANCGCCGNCGTACGCGTANCTACGGCGCGTANCCGCGTANCGCCGCGCGCGTAGCGTANCGCGCGTANCTACGGCGCGTANCAT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m ]\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtest_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_seqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# ==== Attention Visualization ====\u001b[39;00m\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n",
      "Cell \u001b[1;32mIn[8], line 51\u001b[0m, in \u001b[0;36mtest_sequences\u001b[1;34m(sequences, model_path, output_dir, log_dir, save_attention, save_csv)\u001b[0m\n\u001b[0;32m     42\u001b[0m model \u001b[38;5;241m=\u001b[39m CpGPredictor(\n\u001b[0;32m     43\u001b[0m     input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     44\u001b[0m     embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m     dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.33262129231366233\u001b[39m\n\u001b[0;32m     48\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model_path, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[0;32m     53\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Model file './models/bilstm_bahdanau_final.pt' not found."
     ]
    }
   ],
   "source": [
    "test_seqs = [\n",
    "    \"ATGCGCGTANCGCCGNCCGGCGCGTANCTACGGCGCGTANCCGCGTANCGCCGNCCGGCGCGTANCTANCGCGGCGCGTAGCGTANCCGCGTANNCCGCGTANCAT\",\n",
    "    \"CGTANCGCGCGTANCGCCGNCGTACGCGTANCTACGGCGCGTANCCGCGTANCGCCGCGCGCGTAGCGTANCGCGCGTANCTACGGCGCGTANCAT\",\n",
    "]\n",
    "\n",
    "test_sequences(test_seqs)\n",
    "\n",
    "# ==== Attention Visualization ====\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.bar(range(len(attn)), attn)\n",
    "plt.title(f\"Attention Weights for Input Sequence\\nActual: {actual}, Predicted: {pred:.2f}\")\n",
    "plt.xlabel(\"Nucleotide Position\")\n",
    "plt.ylabel(\"Attention Weight\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"attention_sample.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def test_sequences(\n",
    "    sequences,\n",
    "    model_path=r\"./models/bilstm_bahdanau_final.pt\",\n",
    "    output_dir=\"./logs/testing/testing_image\",\n",
    "    log_dir=\"./logs/testing/testing_logs\",\n",
    "    save_attention=True,\n",
    "    save_csv=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Predicts CpG counts for a list of sequences using a pre-trained model and logs results.\n",
    "\n",
    "    Args:\n",
    "        sequences (list[str]): List of DNA sequences.\n",
    "        model_path (str): Path to the saved model.\n",
    "        output_dir (str): Directory to save attention plots.\n",
    "        log_dir (str): Directory to save log file.\n",
    "        save_attention (bool): Whether to save attention bar plots.\n",
    "        save_csv (bool): Whether to save results in CSV format.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    # Setup timestamped logging\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_file = os.path.join(log_dir, f\"test_log_{timestamp}.log\")\n",
    "    logging.basicConfig(\n",
    "        filename=log_file,\n",
    "        filemode=\"w\",\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "\n",
    "    # Load the model\n",
    "    model = CpGPredictor(\n",
    "        input_dim=5,\n",
    "        embedding_dim=64,\n",
    "        hidden_dim=256,\n",
    "        num_layers=2,\n",
    "        dropout=0.33262129231366233\n",
    "    ).to(device)\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file '{model_path}' not found.\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    logging.info(\"✅ Model loaded successfully.\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # For storing attention info for later visualization\n",
    "    all_attentions = []\n",
    "\n",
    "    for idx, seq in enumerate(sequences, 1):\n",
    "        pred, attn = predict_cpg(model, seq, return_attention=True)\n",
    "        actual = count_cpgs(seq)\n",
    "        error = abs(pred - actual)\n",
    "\n",
    "        logging.info(f\"\\nSequence {idx}:\")\n",
    "        logging.info(f\"📌 {seq}\")\n",
    "        logging.info(f\"✅ Actual CpG: {actual}, 🧠 Predicted CpG: {pred:.2f}, ❗ Error: {error:.2f}\")\n",
    "\n",
    "        results.append({\n",
    "            \"Sequence_ID\": f\"Seq_{idx}\",\n",
    "            \"Sequence\": seq,\n",
    "            \"Actual_CpG\": actual,\n",
    "            \"Predicted_CpG\": round(pred, 2),\n",
    "            \"Error\": round(error, 2)\n",
    "        })\n",
    "\n",
    "        # Save attention plot\n",
    "        if save_attention:\n",
    "            plt.figure(figsize=(14, 4))\n",
    "            plt.bar(range(len(attn)), attn)\n",
    "            plt.title(f\"Attention - Seq {idx}\\nActual: {actual}, Predicted: {pred:.2f}\")\n",
    "            plt.xlabel(\"Nucleotide Position\")\n",
    "            plt.ylabel(\"Attention Weight\")\n",
    "            plt.tight_layout()\n",
    "            plot_path = os.path.join(output_dir, f\"attention_seq_{idx}.png\")\n",
    "            plt.savefig(plot_path)\n",
    "            plt.close()\n",
    "            logging.info(f\"🖼️ Attention plot saved to: {plot_path}\")\n",
    "\n",
    "        # Save for inline visualization\n",
    "        all_attentions.append((idx, seq, actual, pred, attn))\n",
    "\n",
    "    # Save results to CSV\n",
    "    if save_csv:\n",
    "        df = pd.DataFrame(results)\n",
    "        csv_path = os.path.join(output_dir, f\"prediction_results_{timestamp}.csv\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        logging.info(f\"📝 Predictions saved to: {csv_path}\")\n",
    "    else:\n",
    "        csv_path = None\n",
    "\n",
    "    print(f\"✅ Testing complete.\\n Logs saved to: {log_file}\\n📄 CSV saved to: {csv_path if save_csv else 'Not saved'}\")\n",
    "\n",
    "    # === Inline attention visualization ===\n",
    "    for idx, seq, actual, pred, attn in all_attentions:\n",
    "        print(f\"\\n📍 Sequence {idx}\")\n",
    "        print(f\"Actual CpG: {actual}, Predicted CpG: {pred:.2f}\")\n",
    "        print(f\"Sequence: {seq}\")\n",
    "\n",
    "        plt.figure(figsize=(14, 4))\n",
    "        plt.bar(range(len(attn)), attn)\n",
    "        plt.title(f\"Attention Weights - Seq {idx}\")\n",
    "        plt.xlabel(\"Nucleotide Position\")\n",
    "        plt.ylabel(\"Attention Weight\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'embedding_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m test_seqs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mATGCGCGTANCGCCGNCCGGCGCGTANCTACGGCGCGTANCCGCGTANCGCCGNCCGGCGCGTANCTANCGCGGCGCGTAGCGTANCCGCGTANNCCGCGTANCAT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCGTANCGCGCGTANCGCCGNCGTACGCGTANCTACGGCGCGTANCCGCGTANCGCCGCGCGCGTAGCGTANCGCGCGTANCTACGGCGCGTANCAT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m ]\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtest_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_seqs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[35], line 42\u001b[0m, in \u001b[0;36mtest_sequences\u001b[1;34m(sequences, model_path, output_dir, log_dir, save_attention, save_csv)\u001b[0m\n\u001b[0;32m     34\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(\n\u001b[0;32m     35\u001b[0m     filename\u001b[38;5;241m=\u001b[39mlog_file,\n\u001b[0;32m     36\u001b[0m     filemode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     37\u001b[0m     level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m )\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCpGPredictor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.33262129231366233\u001b[39;49m\n\u001b[0;32m     48\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'embedding_dim'"
     ]
    }
   ],
   "source": [
    "test_seqs = [\n",
    "    \"ATGCGCGTANCGCCGNCCGGCGCGTANCTACGGCGCGTANCCGCGTANCGCCGNCCGGCGCGTANCTANCGCGGCGCGTAGCGTANCCGCGTANNCCGCGTANCAT\",\n",
    "    \"CGTANCGCGCGTANCGCCGNCGTACGCGTANCTACGGCGCGTANCCGCGTANCGCCGCGCGCGTAGCGTANCGCGCGTANCTACGGCGCGTANCAT\",\n",
    "]\n",
    "\n",
    "test_sequences(test_seqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI Environment",
   "language": "python",
   "name": "ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
